{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPeJ1Nw7XLAG1Aq+9ubl3lA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jashkine/Finetune_llm/blob/main/finetune_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade fsspec datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g--TSjfvgR7i",
        "outputId": "8764260a-5a73-4b16-b237-14ae7f335ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.2)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDzASKpjfhn7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ✅ Load pretrained GPT-2 and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# GPT-2 does not have a padding token by default\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Load a public dataset — we’ll use wikitext\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    result = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n"
      ],
      "metadata": {
        "id": "hxv0ptJ4fmN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-wikitext-results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    fp16=True,  # Use mixed precision if on GPU\n",
        ")\n",
        "\n",
        "# ✅ Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        ")\n"
      ],
      "metadata": {
        "id": "aGl-6mrWgo7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "CnJKrw2LhbNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ✅ Start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "axrsj7algrw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model and tokenizer to a directory\n",
        "save_directory = \"./gpt2-finetuned\"\n",
        "\n",
        "trainer.save_model(save_directory)          # Saves model and config\n",
        "tokenizer.save_pretrained(save_directory)  # Saves tokenizer files too\n"
      ],
      "metadata": {
        "id": "jVCumWkIoelC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate(eval_dataset=tokenized_dataset[\"validation\"])\n",
        "print(results)"
      ],
      "metadata": {
        "id": "tTzymqx-g8P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "eval_loss = results[\"eval_loss\"]\n",
        "perplexity = math.exp(eval_loss)\n",
        "print(f\"Perplexity: {perplexity:.2f}\")\n"
      ],
      "metadata": {
        "id": "dWgLLH9Om6Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
        "test_perplexity = math.exp(test_results[\"eval_loss\"])\n",
        "print(f\"Test Perplexity: {test_perplexity:.2f}\")\n"
      ],
      "metadata": {
        "id": "6f1yYfZmnAPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "# Path to your training log file (adjust if different)\n",
        "log_path = \"./logs/events.out.tfevents...\"\n",
        "\n",
        "# If using tensorboard logs, you can convert them or parse\n",
        "# But simplest: enable Trainer to save logs in JSON\n",
        "\n",
        "# Alternative: if you saved logs manually or have 'trainer.state.log_history':\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "# Extract losses and steps\n",
        "steps = [x['step'] for x in log_history if 'loss' in x]\n",
        "train_losses = [x['loss'] for x in log_history if 'loss' in x]\n",
        "eval_losses = [x['eval_loss'] for x in log_history if 'eval_loss' in x]\n",
        "\n",
        "plt.plot(steps, train_losses, label=\"Train Loss\")\n",
        "if eval_losses:\n",
        "    plt.plot(steps[:len(eval_losses)], eval_losses, label=\"Eval Loss\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Bmshk4MlnKT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Assuming 'model' and 'tokenizer' are your in-memory fine-tuned objects\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "prompt = \"Once upon a time\"\n",
        "outputs = generator(prompt, max_length=50, num_return_sequences=3)\n",
        "\n",
        "for i, output in enumerate(outputs):\n",
        "    print(f\"Generated #{i+1}: {output['generated_text']}\\n\")\n"
      ],
      "metadata": {
        "id": "tJSFJoFTnSvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the model and tokenizer (if you restarted your session)\n",
        "model_name_or_path = \"./gpt2-finetuned\"  # or wherever you saved your fine-tuned model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "prompt = \"Once upon a time\"\n",
        "outputs = generator(prompt, max_length=50, num_return_sequences=3)\n",
        "\n",
        "for i, output in enumerate(outputs):\n",
        "    print(f\"Generated #{i+1}: {output['generated_text']}\\n\")\n"
      ],
      "metadata": {
        "id": "sGkBraREneTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs\n"
      ],
      "metadata": {
        "id": "KklnOMvVoqBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pKre2wp0t-r_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}